{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Content**\n\n<br>\n\n### [1. Libraries Import](#import) ##\n### [2. Read images and Data exploration](#makedf) ## \n### [3. Resampling](#trim) ##\n### [4. Train, test and validation](#generators) ## \n### [5. Training sample visualization](#show) ## \n### [6. Machine Learning Model](#model) ## \n### [7. Custom Keras callback](#callback) ## \n### [8. Instantiate callback](#callbacks) ##\n### [9. Model training](#train) ##\n### [10. Plotting](#plot) ##\n### [11. Prediction and Accuracy assessment](#result) ##\n### [12. Model Export](#save) ##\n\n***","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import\"></a>\n# <center>Import Libraries</center>","metadata":{}},{"cell_type":"code","source":"# data analysis library\nimport numpy as np\nimport pandas as pd\nimport os\nimport math\nimport time\nimport gc\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# plotting and image analysis library\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n# machine learning library\nimport shutil\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model\n\ntf.keras.utils.set_random_seed(1)\ntf.config.experimental.enable_op_determinism()\n\nfrom keras.backend import manual_variable_initialization \nmanual_variable_initialization(True)\n\nfrom numpy.random import seed\nseed(42)\n\nfrom tensorflow import set_random_seed\nset_random_seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"makedf\"></a>\n# <center>Data Import and Exploration</center>","metadata":{}},{"cell_type":"code","source":"pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path=r'../input/paddy-disease-classification/train_images'\ntest_path=r'../input/paddy-disease-classification/test_images'\n\n# read in train.csv\ndf=pd.read_csv(r'../input/paddy-disease-classification/train.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df[['age','variety']]:\n    print(df[col].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sample = {}\n\nfor category in df['label'].unique():\n    shuffle = df.sample(frac=1).reset_index(drop=True)\n    imgs = shuffle[shuffle['label'] == category].values\n    sample = imgs[0:9]\n    all_sample[category] = sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_category(name, f_dict=all_sample):\n    \n    assert isinstance(name, str), \"name must be a string!\"\n    \n    fig, axes = plt.subplots(3,3, figsize=(15, 15))\n    fig.suptitle(name, fontsize=24)\n    \n    try:\n        arr = f_dict[name]\n    except KeyError as e:\n        print(\"Category not exist!\")\n        print(e)\n        \n    f_names = [f[0] for f in arr]\n    f_var = [f[2] for f in arr]\n    f_age = [f[3] for f in arr]\n\n    for n, filename in enumerate(f_names):\n        f = os.path.join(train_path, name, filename)\n        img = mpimg.imread(f)\n        ax = plt.subplot(3, 3, n + 1)\n        ax.imshow(img)\n        ax.set_title(\"{} in {} days\".format(f_var[n], f_age[n]))\n        ax.axis('off')\n\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_category(\"dead_heart\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_category(\"normal\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_category(\"bacterial_leaf_blight\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"resample\"></a>\n# <center>Data Resampling</center>","metadata":{}},{"cell_type":"code","source":"def oversample(df, col='label', n=1500):\n    \"\"\"\n    oversample each label to resolved imbalanced classes\n    \"\"\"\n    for count, i in enumerate(df[col].unique()):\n        df_class = df[df[col] == i]\n        df_class_resample = df_class.sample(n, replace=True)\n        if count == 0:\n            df_resample = df_class_resample\n        else:\n            df_resample = pd.concat([df_class_resample, df_resample], axis=0)\n\n    return df_resample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_resample = oversample(df)\ndf = None\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_resample.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_resample.label.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"split\"></a>\n# <center>Data Split: Training and Validation</center>","metadata":{}},{"cell_type":"code","source":"def create_train_valid(df, train_path, train_size=.9, shuffle=True, random_state=123):\n    \"\"\"\n    create file path and split data\n    \"\"\"\n    if not 'label' in df.columns:\n        print(\"label not found!\")\n        raise\n        \n    if not 'image_id' in df.columns:\n        print(\"image_id not found!\")\n        raise\n    \n    df['filepaths'] = df[['label','image_id']].apply(lambda row: os.path.join(train_path, *row), axis=1)\n    df_new = df[['filepaths','label']]\n    df_new = df_new.rename(columns={\"label\": \"labels\"})\n\n    train_df, valid_df = train_test_split(\n        df_new, \n        train_size=train_size, \n        shuffle=shuffle, \n        random_state=random_state, \n        stratify=df_new['labels'])\n\n    return train_df, valid_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, valid_df = create_train_valid(df_resample, train_path)\nprint('train_df length: ', len(train_df), '\\nvalid_df length: ', len(valid_df))\n\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.reset_index()[['filepaths','labels']]\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_stats(df, n=100, random_state=123):\n    \"\"\"\n    check training image statistics\n    \"\"\"\n    if not 'filepaths' in df.columns:\n        print(\"filepaths not found!\")\n        raise\n        \n    ht = []\n    wt = []\n    sample = df.sample(n=n, random_state=random_state, axis=0)\n\n    for i in range(n):\n        fpath = sample['filepaths'].iloc[i]\n        img = plt.imread(fpath)\n        ht.append(img.shape[0])\n        wt.append(img.shape[1])\n\n    avg_h = np.mean(np.array(ht))\n    avg_w = np.mean(np.array(wt))\n    aspect = round(avg_h/avg_w,2)\n    \n    return avg_h,  avg_w, aspect","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_h, avg_w, aspect = get_stats(train_df)\nprint('average height= ', avg_h, '\\naverage width= ', avg_w, '\\naspect= ', aspect)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"test\"></a>\n# <center>Import Test Data</center>","metadata":{}},{"cell_type":"code","source":"submit_df=pd.read_csv(r'../input/paddy-disease-classification/sample_submission.csv')\ntest_df = submit_df\ntest_df=test_df.drop('label', axis=1)\ntest_df.columns=['filepaths']\ntest_df['filepaths']=test_df['filepaths'].apply(lambda x: os.path.join(test_path,x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"generators\"></a>\n# <center>Create train_gen, test_gen and valid_gen with Data Argumentation</center>","metadata":{}},{"cell_type":"code","source":"from random import choice\n\ndef custom_augmentation(np_tensor):\n    \"\"\"\n    training data argumentation\n    \"\"\"\n    aug_choice = np.random.randint(2, size=10)\n    \n    def cutout(np_tensor):\n        cutout_height = int(np.random.uniform(0.1, 0.2) * np_tensor.shape[0])\n        cutout_width = int(np.random.uniform(0.1, 0.2) * np_tensor.shape[1])\n        cutout_height_point = np.random.randint(np_tensor.shape[0]-cutout_height)\n        cutout_width_point = np.random.randint(np_tensor.shape[1]-cutout_width)\n        np_tensor[cutout_height_point:cutout_height_point+cutout_height, cutout_width_point:cutout_width_point+cutout_width, :] = 127\n        return np_tensor\n\n    def gaussian_noise(np_tensor):\n        mean = 0\n        # variance: randomly between 1 to 25\n        var = np.random.randint(1, 26)\n        # sigma is square root of the variance value\n        noise = np.random.normal(mean,var**0.5,np_tensor.shape)\n        return np.clip(np_tensor + noise, 0, 255).astype('int')\n    \n    def random_crop(np_tensor):\n        # cropped height between 70% to 130%\n        new_height = int(np.random.uniform(0.7, 1.30) * np_tensor.shape[0])\n        # cropped width between 70% to 130%\n        new_width = int(np.random.uniform(0.7, 1.30) * np_tensor.shape[1])\n        # resize to new height and width\n        cropped = tf.image.resize_with_crop_or_pad(np_tensor, new_height, new_width)\n        return tf.image.resize(cropped, np_tensor.shape[:2])\n    \n    def random_contrast(np_tensor):\n        return tf.image.random_contrast(np_tensor, 0.5, 2)\n \n    def random_hue(np_tensor):\n        return tf.image.random_hue(np_tensor, 0.5)\n \n    def random_saturation(np_tensor):\n        return tf.image.random_saturation(np_tensor, 0.2, 3)\n    \n    fns = [random_crop, gaussian_noise, cutout, random_contrast, random_hue, random_saturation]\n    augmnted_tensor = choice(fns)(np_tensor)\n    augmnted_tensor = choice(fns)(augmnted_tensor)\n    \n    return np.array(augmnted_tensor)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training generator with data argumentation\ntrgen=ImageDataGenerator(\n    horizontal_flip=True,\n    rotation_range=30, \n    width_shift_range=.2,\n    height_shift_range=.2,\n    fill_mode=\"wrap\",\n    shear_range=.2,\n    brightness_range=(.5, 1.5),\n    zoom_range=.4,\n    channel_shift_range=30,\n    #rescale=1./255,\n    #preprocessing_function=custom_augmentation\n)\n\n# testing and validation generator with data argumentation\nt_and_v_gen=ImageDataGenerator(\n    horizontal_flip=True,\n    brightness_range=(.7, 1.5),\n    fill_mode=\"wrap\",\n    zoom_range=.2\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"working_dir=r'./'\nimg_size=(320,240)\nbatch_size=20 # for EfficientetB3 model\n\nmsg='{0:70s} for train generator'.format(' ')\nprint(msg, '\\r', end='')\n\ntrain_gen=trgen.flow_from_dataframe(\n    train_df, \n    x_col='filepaths', \n    y_col='labels', \n    target_size=img_size,\n    class_mode='categorical', \n    color_mode='rgb', \n    shuffle=False, \n    batch_size=batch_size\n)\n\nmsg='{0:70s} for valid generator'.format(' ')\nprint(msg, '\\r', end='')\n\nvalid_gen=t_and_v_gen.flow_from_dataframe(\n    valid_df, \n    x_col='filepaths', \n    y_col='labels', \n    target_size=img_size,\n    class_mode='categorical', \n    color_mode='rgb', \n    shuffle=False, \n    batch_size=batch_size\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head().values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for the test_gen we want to calculate the batch size and test steps such that batch_size X test_steps= number of samples in test set\n# this insures that we go through all the sample in the test set exactly once.\n\ntest_batch_size=10\ntest_steps=int(len(test_df)/test_batch_size)\n\nmsg='{0:70s} for test generator'.format(' ')\nprint(msg, '\\r', end='') # prints over on the same line\n\ntest_gen=t_and_v_gen.flow_from_dataframe(\n    test_df, \n    x_col='filepaths', \n    y_col=None, \n    target_size=img_size,\n    class_mode=None, \n    color_mode='rgb', \n    shuffle=False, \n    batch_size=test_batch_size)\n\n# from the generator we can get information we will need later\nclasses=list(train_gen.class_indices.keys())\nclass_indices=list(train_gen.class_indices.values())\nclass_count=len(classes)\n\nprint('\\ntest batch size: {}\\ntest steps: {}\\nnumber of classes: {}'.format(test_batch_size, test_steps, class_count))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"show\"></a>\n# <center>Data Exploration before Training</center>","metadata":{}},{"cell_type":"code","source":"def show_image_samples(gen):\n    t_dict=gen.class_indices\n    classes=list(t_dict.keys())    \n    images,labels=next(gen) # get a sample batch from the generator \n    \n    plt.figure(figsize=(20, 20))\n    r=len(labels) if len(labels)<25 else 25\n\n    for i in range(r):        \n        plt.subplot(5, 5, i + 1)\n        image=images[i] /255       \n        plt.imshow(image)\n        \n        index=np.argmax(labels[i])\n        class_name=classes[index]\n        \n        plt.title(class_name, color='blue', fontsize=14)\n        plt.axis('off')\n        \n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image_samples(train_gen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"model\"></a>\n# <center>Transfer Learning with EfficientNetB3</center>","metadata":{}},{"cell_type":"code","source":"img_shape=(img_size[0], img_size[1], 3)\n# model_name='EfficientNetB3'\n    \nbase_model=tf.keras.applications.efficientnet.EfficientNetB3(\n    include_top=False, \n    weights=\"imagenet\",\n    input_shape=img_shape, \n    pooling='max')\n\n# Note you are always told NOT to make the base model trainable initially but you get better results leaving it trainable\nbase_model.trainable = True\nx = base_model.output\n\n# x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n#                 bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n\nx = BatchNormalization(\n    axis=-1, \n    momentum=0.99, \n    epsilon=0.001)(x)\n\nx = Dense(\n    256, kernel_regularizer=regularizers.l2(0.016),\n    activity_regularizer=regularizers.l1(0.006),\n    bias_regularizer=regularizers.l1(0.006),\n    activation='relu')(x)\n\nx = Dropout(rate=.4, seed=123)(x, training=False)\n\n# x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n#                 bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n\noutput = Dense(class_count, activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=output)\n\nlr = .001 # initiate learning rate\nopt = Adamax(learning_rate=lr)\n\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[\"sparse_categorical_accuracy\"])#['accuracy']) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"callback\"></a>\n# <center>Custom Keras callback</center>\n\nThe LR_ASK callback is a convenient callback that allows you to continue training for ask_epoch more epochs or to halt training.  \nIf you elect to continue training for more epochs you are given the option to retain the current learning rate (LR) or to  \nenter a new value for the learning rate. The form of use is:  \nask=LR_ASK(model,epochs, ask_epoch) where:  \n* model is a string which is the name of your compiled model\n* epochs is an integer which is the number of epochs to run specified in model.fit\n* ask_epoch is an integer. If ask_epoch is set to a value say 5 then the model will train for 5 epochs.  \n  then the user is ask to enter H to halt training, or enter an inter value. For example if you enter 4  \n  training will continue for 4 more epochs to epoch 9 then you will be queried again. Once you enter an  \n  integer value you are prompted to press ENTER to continue training using the current learning rate  \n  or to enter a new value for the learning rate.  \n  \n At the end of training the model weights are set to the weights for the epoch that achieved the lowest validation loss","metadata":{}},{"cell_type":"code","source":"class LR_ASK(keras.callbacks.Callback):\n    def __init__ (self, model, epochs,  ask_epoch):\n        \"\"\"\n        initialize callback\n        \"\"\"\n        super(LR_ASK, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True # query on a specified epoch\n        self.lowest_vloss=np.inf\n        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n        self.best_epoch=1\n        \n        \n    def on_train_begin(self, logs=None):\n        \"\"\"\n        query ask_epoch when training begins\n        \"\"\"\n        if self.ask_epoch == 0:  # adjust ask epoch\n            print('ask_epoch set to 1 instead', flush=True)\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs: # continue training\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n            self.ask=False # do not query\n        if self.epochs == 1:\n            self.ask=False\n        else:\n            print('Training will proceed until epoch', ask_epoch) \n            print('You can then halt training or continue training')  \n        self.start_time= time.time() # set the time at which training started\n        \n    def on_train_end(self, logs=None):\n        \"\"\"\n        measure training time when training completed\n        \"\"\"\n        print('loading model with weights from epoch ', self.best_epoch)\n        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n        hours = tr_duration // 3600\n        minutes = (tr_duration - (hours * 3600)) // 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) # print out training duration time\n        \n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"\n        update validation loss and query again at the end of epoch\n        \"\"\"\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        if v_loss< self.lowest_vloss:\n            self.lowest_vloss=v_loss\n            self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n            self.best_epoch=epoch + 1\n            print (f'\\n validation loss of {v_loss:7.4f} is below lowest loss, saving weights from epoch {str(epoch + 1):3s} as best weights')\n        else:\n            print (f'\\n validation loss of {v_loss:7.4f} is above lowest loss of {self.lowest_vloss:7.4f} keeping weights from epoch {str(self.best_epoch)} as best weights')\n        \n        if self.ask: # query the user\n            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n                ans=input()\n                \n                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True # halt training\n                else: # continue training\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nMaximum epochs specified as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n                    else:\n                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)\n                        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n                        print(f'current LR is  {lr:7.5f}  hit enter to keep  this LR or enter a new LR')\n                        ans=input(' ')\n                        if ans =='':\n                            print (f'keeping current LR of {lr:7.5f}')\n                        else:\n                            new_lr=float(ans)\n                            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) # set the learning rate in the optimizer\n                            print(' changing LR to ', ans)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiate custom callback\n\nepochs=40\nask_epoch=5\nask=LR_ASK(model, epochs,  ask_epoch)\n\ncallbacks=[ask]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"summary\"></a>\n# <center>Model Summary</center>","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"train\"></a>\n# <center>Model Training</center>","metadata":{}},{"cell_type":"code","source":"history=model.fit(x=train_gen, epochs=epochs, verbose=True, callbacks=callbacks, validation_data=valid_gen, validation_steps=None, shuffle=False, initial_epoch=0)\n\n# Epoch 15/40\n# 675/675 [==============================] - 476s 705ms/step - loss: 0.2113 - accuracy: 0.9819 - val_loss: 0.1868 - val_accuracy: 0.9873\n\n#  validation loss of  0.1868 is below lowest loss, saving weights from epoch 15  as best weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"train\"></a>\n# <center>Evaluate the model</center>","metadata":{}},{"cell_type":"code","source":"# get loss and accuracy\neval_dict = model.evaluate(x=train_gen, return_dict=True)\nprint(*eval_dict.items(), sep='\\n')\n\n# 675/675 [==============================] - 347s 514ms/step - loss: 0.1571 - accuracy: 0.9935\n# ('loss', 0.15708895027637482)\n# ('accuracy', 0.993481457233429)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get loss and accuracy\neval_dict_valid = model.evaluate(x=valid_gen, return_dict=True)\nprint(*eval_dict_valid.items(), sep='\\n')\n\n# 75/75 [==============================] - 36s 479ms/step - loss: 0.1883 - accuracy: 0.9873\n# ('loss', 0.18831852078437805)\n# ('accuracy', 0.987333357334137)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"test_model\"></a>\n# <center>Check results from trained model</center>","metadata":{}},{"cell_type":"code","source":"def test_model(ds, model, max_sample=5):\n    \"\"\"\n    display model prediction results\n    \"\"\"\n    #imgs = np.array(ds.filenames)\n    #label = np.array(ds.labels)\n    \n    n = max_sample\n    categories = np.array(list(ds.class_indices.keys()))\n    \n    # display prediction examples in loop\n    for i in range(0,n):\n        imgs,labels = next(ds)\n        preds = model.predict(ds)\n        pred = preds[i]\n        #pred = model.predict(imgs[i])[0]\n        p = np.max(pred)*100\n        pred_cat = categories[np.argmax(pred)]\n        true_cat = categories[ds.labels[i]]\n        print(\"Pred: {0:.2f}% {1:s}\\nTarget: {2:s}\".format(p, pred_cat, true_cat))\n        #plt.imshow(np.array(imgs[i][0], dtype=np.int32))\n        \n        image=imgs[0]/255\n        plt.imshow(image)\n        plt.axis('off')\n        plt.show()\n        time.sleep(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model(valid_gen, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_true = valid_gen.classes\n\npredictions = model.predict(valid_gen)\ny_pred = np.array([np.argmax(x) for x in predictions])\n\ncm = confusion_matrix(y_true, y_pred)\ncm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(valid_gen.classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_gen.class_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"plot\"></a>\n# <center>Visualize training performance</center>","metadata":{}},{"cell_type":"code","source":"def tr_plot(tr_data):\n    \"\"\"\n    Plot training and validation data\n    \"\"\"\n    # get info from model\n    tacc=np.array(tr_data.history['accuracy'])\n    tloss=np.array(tr_data.history['loss'])\n    vacc=np.array(tr_data.history['val_accuracy'])\n    vloss=np.array(tr_data.history['val_loss']) \n    \n    # compute statistics\n    val_lowest, index_loss = np.min(vloss), np.argmin(vloss)+1\n    acc_highest, index_acc = np.max(vacc), np.argmax(vacc)+1\n    \n    # prepare plotting data and labels\n    sc_label='best epoch= {}'.format(index_loss+1)\n    vc_label='highest acc= {}%'.format(round(acc_highest*100,1))\n    Epochs=np.linspace(1,len(tacc),len(tacc))\n\n    with plt.style.context('ggplot'): # define plotting style\n        fig,(ax0, ax1) = plt.subplots(1, 2, figsize=(16,6))\n        \n        # plot Training and Validation Loss\n        ax0.plot(Epochs,tloss, 'r', label='Training loss', linewidth=2, alpha=.8)\n        ax0.plot(Epochs,vloss,'g',label='Validation loss', linewidth=2, alpha=.8)\n        ax0.scatter(index_loss,val_lowest, s=260, c='blue', label=sc_label, marker=(5, 1))\n        ax0.set_title('Training and Validation Loss', fontsize=22)\n        ax0.set_xlabel('Epochs', fontsize=18)\n        ax0.set_ylabel('Loss', fontsize=18)\n        ax0.legend()\n        \n        # plot Training and Validation Accuracy\n        ax1.plot (Epochs,tacc,'r',label='Training Accuracy', linewidth=2, alpha=.8)\n        ax1.plot (Epochs,vacc,'g',label='Validation Accuracy', linewidth=2, alpha=.8)\n        ax1.scatter(index_acc,acc_highest, s=260, c='blue', label=vc_label, marker=(5, 1))\n        ax1.set_title('Training and Validation Accuracy', fontsize=22)\n        ax1.set_xlabel('Epochs', fontsize=18)\n        ax1.set_ylabel('Accuracy', fontsize=18)\n        ax1.legend()\n        \n        # general layout\n        ax0.grid(True, which='both', alpha=.3, c=\"gray\")\n        ax0.minorticks_on()\n        \n        ax1.grid(True, which='both', alpha=.3, c=\"gray\")\n        ax1.minorticks_on()\n        \n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_plot(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"report\"></a>\n# <center>Classification Report</center>","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(data, labels, filename, save=False):\n    \"\"\"Plot confusion matrix using heatmap.\n \n    Args:\n        data (list of list): List of lists with confusion matrix data.\n        labels (list): Labels which will be plotted across x and y axis.\n        output_filename (str): Path to output file.\n \n    \"\"\"\n    sns.set(color_codes=True)\n    sns.set(font_scale=1.4)\n    \n    plt.figure(1, figsize=(9, 6))\n    plt.title(\"Confusion Matrix\")\n \n    ax = sns.heatmap(data, annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Scale'})\n    ax.set_xticklabels(labels)\n    ax.set_yticklabels(labels)\n    ax.set(ylabel=\"True Label\", xlabel=\"Predicted Label\")\n    \n    if save == True:\n        plt.savefig(filename, bbox_inches='tight', dpi=300)\n\n# create confusion matrix\nplot_confusion_matrix(cm, classes, \"confusion_matrix.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"result\"></a>\n# <center>Prediction on the Testing Data</a>\n\n#### Define a function which generates predictions on the test set from a test generator","metadata":{}},{"cell_type":"code","source":"# {'bacterial_leaf_blight': 0,\n#  'bacterial_leaf_streak': 1,\n#  'bacterial_panicle_blight': 2,\n#  'blast': 3,\n#  'brown_spot': 4,\n#  'dead_heart': 5,\n#  'downy_mildew': 6,\n#  'hispa': 7,\n#  'normal': 8,\n#  'tungro': 9}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictor(model, data, cat, batch_size=10, write_csv=False, verbose=True):\n    \"\"\"\n    use model to predict image class\n    \"\"\"\n    \n    if verbose:\n        preds = model.predict(data, verbose=1, batch_size=batch_size)\n        count = len(cat)\n        print(\"Predict images to {} classes.\".format(count))\n        print(cat)\n    else:\n        preds = model.predict(data, verbose=0, batch_size=batch_size)\n    \n    label = []\n    image_id = data.filenames\n    \n    for i in range(preds.shape[0]):\n        idx = np.argmax(preds[i])\n        pred = classes[idx]\n        label.append(pred)\n    \n    if verbose == 1:\n        print(\"Completed {} predictions.\".format(i+1))\n    \n    try:\n        Idseries = pd.Series(image_id, name='image_id')\n        Lseries = pd.Series(label, name='label')\n        df = pd.concat([Idseries,Lseries], axis=1)\n    except:\n        print(\"Dataframe creation failed!\")\n        raise\n    \n    if write_csv == True:\n        csv_path=os.path.join(working_dir, 'submit.csv')\n        df.to_csv(csv_path, index=False)\n        if verbose == 1:\n            print(\"CSV file successfully exported!\")\n    \n    return df\n\n# machine learning model prediction\nclasses = list(train_gen.class_indices.keys())\n\ndf = predictor(model=model, data=test_gen, cat=classes)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(df.label.values == np.array(result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.where(df.label.values != np.array(result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(test_gen, verbose=1, batch_size=20)#10)\n\nresult = []\nfor i in range(preds.shape[0]):\n    id = np.argmax(preds[i])\n    pred = classes[id]\n    result.append(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(test_gen, verbose=1, batch_size=batch_size)\n\nlabel = []\nimage_id = test_gen.filenames\n    \nfor i in range(preds.shape[0]):\n    idx = np.argmax(preds[i])\n    pred = classes[idx]\n    label.append(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.where(np.array(label) != result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_1 = model.predict(test_gen, verbose=1, batch_size=batch_size)\n\npred_2 = model.predict(test_gen, verbose=1, batch_size=batch_size)\n\nprint(np.array_equal(pred_1, pred_2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.label.values[:10])\nprint(result[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(test_gen, verbose=1, batch_size=10)\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = []\nfor i in range(preds.shape[0]):\n    id = np.argmax(preds[i])\n    pred = classes[id]\n    result.append(pred)\n\nresult","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv(\"submit.csv\")\nsubmit.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit['label'] = result\nsubmit.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.to_csv(\"submit_fix\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"save\"></a>\n# <center>Save the model","metadata":{}},{"cell_type":"code","source":"# loading saved model\n#loaded_model = keras.models.load_model(\"../input/paddy-model/paddy_disease_model_v2.h5\")\n\nmodel_new = tf.keras.models.model_from_json(model_json)\nmodel_new.compile()\nmodel_new.load_weights()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check working dir\npwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list files\nls","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}